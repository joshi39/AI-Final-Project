{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import imageio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, RepeatedStratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\n",
    "from keras.applications import resnet\n",
    "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB4, EfficientNetB5, EfficientNetB6, EfficientNetB7\n",
    "from keras.applications.resnet import ResNet50\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import load_img, img_to_array, array_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = r'/kaggle/input/the-iq-othnccd-lung'\n",
    "\n",
    "categories = ['Bengin cases', 'Malignant cases', 'Normal cases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/kaggle/input/the-iq-othnccd-lung\\\\Bengin cases'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m class_num \u001b[39m=\u001b[39m categories\u001b[39m.\u001b[39mindex(i)\n\u001b[0;32m      5\u001b[0m temp_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(path):\n\u001b[0;32m      7\u001b[0m     filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, file)\n\u001b[0;32m      8\u001b[0m     height, width, channels \u001b[39m=\u001b[39m imageio\u001b[39m.\u001b[39mimread(filepath)\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/kaggle/input/the-iq-othnccd-lung\\\\Bengin cases'"
     ]
    }
   ],
   "source": [
    "size_data = {}\n",
    "for i in categories:\n",
    "    path = os.path.join(directory, i)\n",
    "    class_num = categories.index(i)\n",
    "    temp_dict = {}\n",
    "    for file in os.listdir(path):\n",
    "        filepath = os.path.join(path, file)\n",
    "        height, width, channels = imageio.imread(filepath).shape\n",
    "        if str(height) + ' x ' + str(width) in temp_dict:\n",
    "            temp_dict[str(height) + ' x ' + str(width)] += 1 \n",
    "        else:\n",
    "            temp_dict[str(height) + ' x ' + str(width)] = 1\n",
    "    \n",
    "    size_data[i] = temp_dict\n",
    "        \n",
    "size_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categories:\n",
    "    path = os.path.join(directory, i)\n",
    "    class_num = categories.index(i)\n",
    "    for file in os.listdir(path):\n",
    "        filepath = os.path.join(path, file)\n",
    "        print(i)\n",
    "        img = cv2.imread(filepath, 0)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "for i in categories:\n",
    "    cnt, samples = 0, 3\n",
    "    fig, ax = plt.subplots(samples, 3, figsize=(15, 15))\n",
    "    fig.suptitle(i)\n",
    "    \n",
    "    path = os.path.join(directory, i)\n",
    "    class_num = categories.index(i)\n",
    "    for curr_cnt, file in enumerate(os.listdir(path)):\n",
    "        filepath = os.path.join(path, file)\n",
    "        img = cv2.imread(filepath, 0)\n",
    "        \n",
    "        img0 = cv2.resize(img, (img_size, img_size))\n",
    "        \n",
    "        img1 = cv2.GaussianBlur(img0, (5, 5), 0)\n",
    "        \n",
    "        ax[cnt, 0].imshow(img)\n",
    "        ax[cnt, 1].imshow(img0)\n",
    "        ax[cnt, 2].imshow(img1)\n",
    "        cnt += 1\n",
    "        if cnt == samples:\n",
    "            break\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "img_size = 256\n",
    "\n",
    "for i in categories:\n",
    "    path = os.path.join(directory, i)\n",
    "    class_num = categories.index(i)\n",
    "    for file in os.listdir(path):\n",
    "        filepath = os.path.join(path, file)\n",
    "        img = cv2.imread(filepath, 0)\n",
    "        # preprocess here\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        data.append([img, class_num])\n",
    "        \n",
    "random.shuffle(data)\n",
    "\n",
    "X, y = [], []\n",
    "for feature, label in data:\n",
    "    X.append(feature)\n",
    "    y.append(label)\n",
    "    \n",
    "print('X length:', len(X))\n",
    "print('y counts:', Counter(y))\n",
    "\n",
    "# normalize\n",
    "X = np.array(X).reshape(-1, img_size, img_size, 1)\n",
    "X = X / 255.0\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=10, stratify=y)\n",
    "\n",
    "print(len(X_train), X_train.shape)\n",
    "print(len(X_valid), X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(y_train), Counter(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), X_train.shape)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_size*img_size*1)\n",
    "\n",
    "print(len(X_train), X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before SMOTE:', Counter(y_train))\n",
    "smote = SMOTE()\n",
    "X_train_sampled, y_train_sampled = smote.fit_resample(X_train, y_train)\n",
    "print('After SMOTE:', Counter(y_train_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], img_size, img_size, 1)\n",
    "X_train_sampled = X_train_sampled.reshape(X_train_sampled.shape[0], img_size, img_size, 1)\n",
    "\n",
    "print(len(X_train), X_train.shape)\n",
    "print(len(X_train_sampled), X_train_sampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create the model\n",
    "def create_model(filters, kernel_size, optimizer, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters, kernel_size, input_shape=X_train.shape[1:], activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters, kernel_size, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the Keras classifier for use in scikit-learn\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'batch_size': [8, 16, 32],\n",
    "    'epochs': [10, 20, 30],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],\n",
    "    'filters': [32, 64, 128],\n",
    "    'kernel_size': [(3, 3), (5, 5), (7, 7)],\n",
    "}\n",
    "\n",
    "# Perform randomized search over the hyperparameters\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=10, cv=3)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Create the model with the best hyperparameters\n",
    "model = create_model(**random_search.best_params_)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_sampled, y_train_sampled, validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Conv2D(64, (3, 3), input_shape=X_train.shape[1:]))\n",
    "model1.add(Activation('relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(16))\n",
    "model1.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model1.fit(X_train_sampled, y_train_sampled, batch_size=8, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model1.predict(X_valid, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_valid, y_pred_bool))\n",
    "\n",
    "print(confusion_matrix(y_true=y_valid, y_pred=y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv2D(64, (3, 3), input_shape=X_train.shape[1:]))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model2.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(16))\n",
    "model2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = {\n",
    "    0: X_train.shape[0]/(3*Counter(y_train)[0]),\n",
    "    1: X_train.shape[0]/(3*Counter(y_train)[1]),\n",
    "    2: X_train.shape[0]/(3*Counter(y_train)[2]),\n",
    "}\n",
    "\n",
    "# new_weights[0] = 0.5\n",
    "# new_weights[1] = 20\n",
    "\n",
    "new_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(X_train, y_train, batch_size=8, epochs=10, validation_data=(X_valid, y_valid), class_weight=new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(X_valid, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_valid, y_pred_bool))\n",
    "\n",
    "print(confusion_matrix(y_true=y_valid, y_pred=y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Train')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True) \n",
    "val_datagen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=8) \n",
    "val_generator = val_datagen.flow(X_valid, y_valid, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv2D(64, (3, 3), input_shape=X_train.shape[1:]))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model3.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(16))\n",
    "model3.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model3.fit_generator(train_generator, epochs=5, validation_data=val_generator, class_weight=new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model3.predict(X_valid, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_valid, y_pred_bool))\n",
    "\n",
    "print(confusion_matrix(y_true=y_valid, y_pred=y_pred_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
